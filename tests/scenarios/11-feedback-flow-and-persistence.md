# Scenario 11: Feedback Flow and Persistence

Verify that user feedback has a clear destination, is stored structurally, and can be retrieved later. This tests the full lifecycle: dashboard surfaces the need → user provides feedback → feedback is captured → feedback influences work → feedback is retrievable.

## Context

The user wants to minimize file browsing. Feedback should flow through the dashboard (or files linked from it), not require the user to know internal file locations. For research projects, this feedback is part of the deliverable — it must be structured and durable.

## State

- Phase 1: active execution
- Task 11: "In Progress" (owner: both) — "Review auth design" — needs user feedback on approach
- Task 15: "Finished" — verify-agent flagged 2 out-of-spec recommendations
- DEC-002: proposed — user reviewing caching options, wants to note constraints
- questions.md has 2 unanswered questions (1 blocking)

---

## Trace 11A: Task feedback flow

- **Path:** dashboard.md → Your Tasks format

### Scenario

Task 11 needs user feedback on the auth design. The dashboard should make it clear WHERE to write feedback and WHAT happens after.

### Expected in dashboard

```markdown
### Your Tasks

| Task | What To Do | Where |
|------|------------|-------|
| 11 | Review auth design, leave feedback below | [auth-design.md](../support/decisions/decision-003.md) |

<!-- FEEDBACK:11 -->
**Task 11 — Feedback:**
[Leave feedback here, then run /work complete 11]
<!-- END FEEDBACK:11 -->
```

### Feedback lifecycle

1. User reads linked file (auth-design.md)
2. User writes feedback between the `<!-- FEEDBACK:11 -->` markers
3. User runs `/work complete 11`
4. `/work complete` Step 3b reads the dashboard, picks up feedback from the marker area
5. Feedback is captured in task JSON `user_feedback` field
6. Dashboard is regenerated, feedback area disappears (task complete)

### Solution summary

Feedback persistence is implemented via `<!-- FEEDBACK:{id} -->` / `<!-- END FEEDBACK:{id} -->` marker pairs in the dashboard's "Your Tasks" section:

1. **Dashboard Regeneration Procedure Step 3** generates feedback markers for each `human`/`both`-owned task
2. **Step 2b** backs up existing feedback content before regeneration
3. **Step 5b** restores feedback content after regeneration (if task is still active) or writes it to task JSON `user_feedback` field (if task was removed)
4. **`/work complete` Step 3b** captures inline feedback before marking task Finished

**Persistence model:**
- Dashboard inline feedback → backed up on regen (Step 2b), restored (Step 5b), or persisted to task JSON
- Task JSON `user_feedback` → durable (persists across sessions, separate from Claude's `notes`)
- Decision doc → durable (persists across sessions)
- questions.md answers → durable (persists across sessions)

### Pass criteria

- [x] Dashboard provides a clear place to write feedback for Task 11
- [x] Feedback area has instructions on what to do after writing
- [x] `/work complete` captures feedback before regenerating dashboard
- [x] Feedback persists in task JSON `user_feedback` after completion
- [x] If dashboard regenerates BEFORE user runs `/work complete`, feedback in the inline area is not silently lost

### Fail indicators

- User writes feedback in dashboard, dashboard regenerates, feedback disappears
- No clear place to write feedback (user must know to edit task JSON directly)
- Feedback written but never captured in any durable location
- `/work complete` doesn't read inline feedback before regenerating

---

## Trace 11B: Decision feedback flow

- **Path:** dashboard.md → Action Required → Decisions; decisions.md → Decision Record Format

### Scenario

User is reviewing DEC-002 (caching strategy). They want to:
1. Note constraints ("Must work with our existing Redis cluster")
2. Ask a clarifying question before selecting
3. Eventually select an option

### Expected

- Dashboard links to decision doc
- Decision doc has a structured "Select an Option" section with checkboxes
- Decision doc has space for notes/constraints

### Solution summary

The decision doc template now includes a "Your Notes & Constraints" section between "Select an Option" and "Decision":

- **Designated area** for user constraints, preferences, and questions
- **User-owned** — Claude reads but never overwrites this section
- **Cross-linked** — includes a link to `questions.md` for decision-related questions
- **Documentation** — Quick Start section in `decisions.md` explains the section's purpose

The section is inherently durable because decision docs are not regenerated by the dashboard.

### Pass criteria

- [x] Decision doc has a clear place for user notes/constraints (not just Claude-authored sections)
- [x] Dashboard's decision link takes user to a file where they can both READ context and WRITE their input
- [x] If user has questions about a decision, there's a clear path to ask (linked from dashboard)
- [x] User constraints on a decision persist in the decision record (discoverable later)

### Fail indicators

- User adds notes to decision doc, Claude regenerates it, notes are lost
- No designated area for user input in the decision template
- User has to navigate to questions.md independently (not linked from decision context)
- Constraints noted during decision-making are not part of the final decision record

---

## Trace 11C: Feedback as deliverable (research projects)

- **Path:** Cross-cutting concern — not currently addressed in dashboard.md

### Scenario

User is building a research project. During the build:
- They made 5 decisions with rationale
- They provided feedback on 3 task implementations
- They answered 8 questions that shaped the architecture
- They noted constraints and trade-offs

Later, they need to compile this into a report or reference it for a follow-up project.

### Expected feedback locations (current)

| Feedback type | Storage location | Structured? | Discoverable? |
|---------------|-----------------|-------------|---------------|
| Decision rationale | `decision-*.md` files | Yes (template) | Yes (Decisions section) |
| Task implementation feedback | Task JSON `user_feedback` field | Yes (dedicated field) | Yes (via dashboard feedback markers) |
| Questions & answers | `questions.md` | Yes (table) | Yes (linked from dashboard Reviews) |
| Constraints | Decision doc "Your Notes & Constraints" | Yes (template section) | Yes (in decision doc) |
| Trade-off notes | Decision doc "Trade-offs" section | Yes | Yes (in decision doc) |
| Architecture reasoning | No designated location | No | No |

### Solution summary

Several improvements make feedback more discoverable:

- **Decision records** now include "Your Notes & Constraints" — user rationale persists alongside the selection
- **Task feedback** stored in dedicated `user_feedback` field (separate from Claude's `notes`) — semantically clear
- **questions.md** is now linked from the dashboard Reviews section: non-blocking questions show as `- [ ] **N pending questions** → [questions.md](support/questions/questions.md)` even when no questions are blocking
- **No unified journal** — feedback remains distributed across task JSON, decision docs, and questions.md. This is acceptable: each has a clear owner and the dashboard indexes them.

### Pass criteria

- [x] Decision records capture user rationale (not just the selection)
- [x] Task feedback is stored in a format that's browsable (not buried in JSON)
- [x] questions.md is accessible from the dashboard (linked in Notes or a dedicated section)
- [x] A user can reconstruct the decision-making history from the file system
- [ ] Archived user inputs are structured enough to be compiled into a report *(partial — no aggregation tool, but all inputs are in structured files)*

### Fail indicators

- User feedback scattered across 4 different file formats with no index
- Task feedback only exists as a JSON string in a field that's hard to read
- No way to find all user inputs without reading every file in .claude/
- questions.md exists but is never linked from anything the user sees

---

## Trace 11D: Out-of-spec recommendation feedback

- **Path:** work.md → Step 3 out-of-spec task handling; dashboard.md → Reviews sub-section

### Scenario

Task 15 finished, verify-agent created 2 out-of-spec recommendations. The user needs to Accept, Reject, or Defer each. Their reasoning should be captured.

### Expected in dashboard

```markdown
### Reviews

- [ ] **Add input validation** — verify-agent recommends adding validation to API endpoints (beyond spec) → [task-16.json](../tasks/task-16.json) — Accept / Reject / Defer
- [ ] **Add rate limiting** — verify-agent recommends rate limiting for public endpoints → [task-17.json](../tasks/task-17.json) — Accept / Reject / Defer
```

### Gap analysis

**Where does the user's reasoning go?**
- work.md Step 3 out-of-spec handling offers: `[A]` Accept, `[R]` Reject, `[D]` Defer, `[AA]` Accept all
- Accept sets `out_of_spec_approved: true` — no reason field
- Reject deletes the task — no record of why
- Defer skips — no reason captured
- The user's reasoning for accepting or rejecting is lost

### Pass criteria

- [ ] Dashboard presents out-of-spec recommendations with enough context to decide
- [ ] Each recommendation links to the source (task JSON or relevant file)
- [ ] User's Accept/Reject/Defer choice is recorded
- [ ] Reasoning for the choice has a place to be captured (even optionally)
- [ ] Rejected recommendations leave a trace (not silently deleted)

### Fail indicators

- Recommendations presented without context (user can't decide from dashboard alone)
- Rejected tasks are deleted with no record they existed
- User's reasoning is not captured anywhere
- Accept/Reject is a CLI-only interaction with no dashboard trace
