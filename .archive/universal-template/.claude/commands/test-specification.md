# Command: test-specification

## Purpose
Generate and execute specification validation tests to find issues, gaps, and inconsistencies before implementation begins.

## When to Use
- User says they're "done" or "mostly done" with the specification
- User wants to validate a specific section of the specification
- After making major changes to the specification
- Before running /sync-from-planning to pull spec to main project

## Prerequisites
- `planning/specification.md` exists and has content
- `planning/.claude/context/phases.md` is defined
- User is ready for validation (spec is in reviewable state)

## Process

### Step 1: Determine Scope
Ask user: "What would you like to test?"

Options:
1. **Full specification** - Generate tests for all sections (comprehensive)
2. **Specific sections** - User specifies which sections (e.g., "Section 3 and 4")
3. **Specific aspect** - User specifies test type (e.g., "just user flows")
4. **Run existing tests** - Re-run previously generated tests

### Step 2: Read Specification
Read `planning/specification.md` completely

Analyze:
- Identify major sections
- Identify user flows
- Identify business logic
- Identify phase dependencies
- Identify decision references

### Step 3: Generate Test Suite
Based on scope, create test files in `planning/tests/`:

#### For Full Specification:
Create tests for:
- Each major user flow (test type: user_flow)
- Each business logic section (test type: logic_validation)
- Cross-section consistency (test type: consistency_check)
- Completeness (test type: completeness_check)

#### Test File Naming:
- `spec-test-001-registration-flow.md`
- `spec-test-002-payment-logic.md`
- `spec-test-003-phase-consistency.md`
- `spec-test-004-completeness.md`

Show user generated test list:
```
Generated test suite (4 tests):
1. spec-test-001: User registration flow validation
2. spec-test-002: Payment calculation logic validation
3. spec-test-003: Phase input/output consistency check
4. spec-test-004: Specification completeness check

Would you like to run all tests, or select specific ones?
```

### Step 4: Execute Tests
For each test selected:

#### 4.1 Load Test Definition
Read test file from `planning/tests/spec-test-NNN.md`

#### 4.2 Execute with Specification Architect Agent
Use Task tool with agent_type="specification-architect":
```
Prompt:
You are validating a project specification.

Specification: [content from planning/specification.md sections referenced in test]

Test Definition: [test file content]

Your task:
1. Read the specification sections in scope
2. Execute each test case:
   - Trace through the steps
   - Check if specification defines behavior for each step
   - Check for contradictions or ambiguities
   - Determine pass/fail for each test case
3. Identify issues:
   - Note severity (critical, major, minor)
   - Note location in specification
   - Suggest fixes
4. Fill in test results:
   - Actual outcomes for each test case
   - Pass/fail determination
   - Issues found list
5. Return completed test file content

Be thorough but practical. Focus on issues that would block or complicate implementation.
```

#### 4.3 Parse Agent Results
Agent returns:
- Completed test file with results filled in
- Overall test status (passed/failed/needs_review)
- List of issues found

#### 4.4 Update Test File
Write updated test file with results to `planning/tests/spec-test-NNN.md`

### Step 5: Generate Tasks for Issues
For each issue found across all tests:

Create task in `planning/.claude/tasks/task-*.json`:
```json
{
  "id": "task-[next-id]",
  "title": "Fix: [Issue Title]",
  "description": "[Issue Description]\n\nLocation: [Spec Section]\nSeverity: [Severity]\nTest: [Test ID]\n\nSuggested Fix:\n[Fix Description]",
  "status": "pending",
  "difficulty": [
    critical → 7-8,
    major → 5-6,
    minor → 3-4
  ],
  "priority": "[severity]",
  "created": "[date]",
  "updated": "[date]",
  "parent_task": null,
  "subtasks": [],
  "related_phases": ["phase-N"],
  "related_decisions": ["decision-N"],
  "assigned_agent": null,
  "validation": {
    "criteria": [
      "Update planning/specification.md per suggested fix",
      "Re-run [test-id] and verify it passes"
    ],
    "completed": false
  },
  "notes": "Generated by /test-specification from [test-id]"
}
```

Difficulty scoring:
- **Critical severity**: 7-8 (requires careful specification revision)
- **Major severity**: 5-6 (significant ambiguity to resolve)
- **Minor severity**: 3-4 (simple clarification)

### Step 6: Create Test Summary Report
Create `planning/test-results-summary.md`:

```markdown
# Specification Test Results

## Test Execution Summary
- **Date**: [timestamp]
- **Tests Run**: [N]
- **Tests Passed**: [N]
- **Tests Failed**: [N]
- **Tests Needing Review**: [N]

## Test Results

### spec-test-001: [Test Name]
- **Status**: passed | failed | needs_review
- **Issues Found**: [N]
- **Generated Tasks**: [task-XXX, task-YYY]

### spec-test-002: [Test Name]
- **Status**: [status]
- **Issues Found**: [N]
- **Generated Tasks**: [task-ZZZ]

## Issues Summary
Total issues found: [N]

### Critical Issues ([N])
- Issue from spec-test-001: [Brief description] → task-XXX
- Issue from spec-test-003: [Brief description] → task-YYY

### Major Issues ([N])
[List]

### Minor Issues ([N])
[List]

## Generated Tasks
Total tasks created: [N]

Run `/sync-tasks` to see task overview, or view individual tasks in `.claude/tasks/`

Priority:
- Critical: [N] tasks
- High: [N] tasks
- Medium: [N] tasks
- Low: [N] tasks

## Recommendations
[Agent recommendations for improving specification before implementation]

## Next Steps
1. Review generated tasks
2. Complete high-priority tasks to fix critical issues
3. Update specification based on task resolutions
4. Re-run tests to verify fixes: `/test-specification --rerun`
5. When all tests pass, run `/sync-from-planning` to proceed to implementation

## Test Details
See individual test files in `planning/tests/` for detailed test case results.
```

### Step 7: Present Results to User
Show user:
```
Specification Testing Complete!

Results:
✓ Tests Passed: 2
✗ Tests Failed: 1
⚠ Tests Needing Review: 1

Issues Found:
- Critical: 1
- Major: 3
- Minor: 2

Generated Tasks: 6
- task-101: Fix duplicate email error handling (Priority: critical)
- task-102: Clarify password validation rules (Priority: high)
- task-103: Add error message specifications (Priority: medium)
- [...]

Full report: planning/test-results-summary.md
Detailed tests: planning/tests/spec-test-*.md

Next steps:
1. Review tasks in planning/.claude/tasks/
2. Complete critical/high priority tasks first
3. Re-run tests after fixes: /test-specification --rerun
4. When ready, proceed to implementation: /sync-from-planning

Would you like to see the detailed results, or should I run /sync-tasks to show the task list?
```

## Output Location
- `planning/tests/spec-test-*.md` - Individual test files with results
- `planning/.claude/tasks/task-*.json` - Generated refinement tasks
- `planning/test-results-summary.md` - Overall test summary

## Options/Flags

### --rerun
Re-run existing tests without generating new ones
```
/test-specification --rerun
```

### --section [section-number]
Test only a specific section
```
/test-specification --section 3
```

### --type [test-type]
Generate only specific type of tests
```
/test-specification --type user_flow
```

## Example Interaction

```
User: /test-specification

Claude: I'll validate your specification. What would you like to test?

1. Full specification (comprehensive - all sections)
2. Specific sections (you choose which ones)
3. Specific test type (e.g., just user flows or logic validation)
4. Re-run existing tests

User: Full specification

Claude: Reading planning/specification.md...

I've identified these areas to test:
- User registration flow (Section 3.2)
- Payment calculation logic (Section 4.3)
- Phase input/output consistency (Section 2)
- Overall completeness

I'll create 4 tests. This will take a moment...

[Generates test files]

Generated test suite:
✓ spec-test-001: User registration flow validation
✓ spec-test-002: Payment calculation logic validation
✓ spec-test-003: Phase input/output consistency check
✓ spec-test-004: Specification completeness check

Running tests with specification-architect agent...

[Executes tests]

Test Results:
✓ spec-test-001: PASSED (registration flow is complete)
✗ spec-test-002: FAILED (2 issues found in payment logic)
✓ spec-test-003: PASSED (phases are consistent)
⚠ spec-test-004: NEEDS REVIEW (some gaps identified)

Generating tasks for issues found...

Created 6 tasks:
- task-101: Fix payment discount stacking ambiguity (Priority: critical)
- task-102: Specify payment rounding behavior (Priority: high)
- task-103: Add error messages specification (Priority: medium)
- task-104: Define API rate limiting strategy (Priority: medium)
- task-105: Clarify user role inheritance (Priority: low)
- task-106: Add deployment requirements section (Priority: low)

Summary report: planning/test-results-summary.md

Next steps:
1. Review and complete task-101 (critical) and task-102 (high)
2. Re-run tests after fixes: /test-specification --rerun
3. When tests pass, proceed to implementation: /sync-from-planning

Would you like to see the task list?

User: Yes, show me the tasks

Claude: [Runs /sync-tasks to show task overview]
```

## Notes
- Tests are saved in `planning/tests/` and can be re-run multiple times
- Each test run overwrites previous results in test files
- Tasks are cumulative (new test runs add tasks, don't replace)
- Use `--rerun` to avoid generating duplicate tests
- Tests are meant to find issues, not be exhaustive QA (focus on high-impact areas)
- Specification Architect agent looks for implementation blockers, not perfection
