name: Test Scripts CI/CD

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'scripts/**'
      - '.github/workflows/test-scripts.yml'
  pull_request:
    branches: [ main ]
    paths:
      - 'scripts/**'

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: ['3.8', '3.9', '3.10', '3.11']

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Cache dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('scripts/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scripts/requirements.txt
        pip install pytest pytest-cov flake8 mypy

    - name: Lint with flake8
      run: |
        # Stop build if there are Python syntax errors or undefined names
        flake8 scripts/ --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 scripts/ --count --exit-zero --max-complexity=10 --max-line-length=120 --statistics

    - name: Type check with mypy
      run: |
        mypy scripts/ --ignore-missing-imports || true

    - name: Test template detection
      run: |
        cd scripts
        python test-templates.py

    - name: Test task manager
      run: |
        cd scripts
        python task-manager.py validate
        python task-manager.py list

    - name: Test validation gates
      run: |
        cd scripts
        python validation-gates.py validate-all --json

    - name: Test schema validator
      run: |
        cd scripts
        python schema-validator.py health --json

    - name: Test CLI wrapper
      run: |
        cd scripts
        python claude-cli.py --version
        python claude-cli.py task list --json
        python claude-cli.py metrics health --json

    - name: Run pytest with coverage
      run: |
        cd scripts
        pytest test-templates.py -v --cov=. --cov-report=xml --cov-report=term || true

    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./scripts/coverage.xml
        flags: unittests
        name: codecov-umbrella
        fail_ci_if_error: false

  integration:
    runs-on: ubuntu-latest
    needs: test

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scripts/requirements.txt

    - name: Integration test - Bootstrap workflow
      run: |
        # Create test specification
        cat > test_spec.md << EOF
        # Test Project

        ## Requirements
        - Build REST API with authentication
        - Create database schema
        - Implement data pipeline
        EOF

        # Test bootstrap
        cd scripts
        python bootstrap.py detect --spec ../test_spec.md
        python bootstrap.py bootstrap --spec ../test_spec.md --output ../test_output

        # Verify output
        test -d ../test_output/.claude
        test -f ../test_output/CLAUDE.md
        test -f ../test_output/README.md

    - name: Integration test - Task workflow
      run: |
        cd scripts

        # Create test task
        cat > test_task.json << EOF
        {
          "id": "test_1",
          "title": "Test Task",
          "description": "Integration test task",
          "difficulty": 5,
          "status": "Pending",
          "created_date": "2025-12-17",
          "updated_date": "2025-12-17",
          "dependencies": [],
          "confidence": 75
        }
        EOF

        # Create tasks directory
        mkdir -p ../.claude/tasks
        cp test_task.json ../.claude/tasks/task-test_1.json

        # Test operations
        python task-manager.py validate --task-id test_1
        python validation-gates.py pre --task-id test_1 --json
        python task-manager.py sync

    - name: Integration test - Analysis workflow
      run: |
        cd scripts

        # Test analysis operations
        python dependency-analyzer.py cycles --json
        python metrics-dashboard.py health --json
        python pattern-matcher.py suggest-breakdown --text "implement authentication API"

    - name: Integration test - CLI workflow
      run: |
        cd scripts

        # Test unified CLI
        python claude-cli.py task list --json
        python claude-cli.py validate all
        python claude-cli.py analyze cycles
        python claude-cli.py metrics health
        python claude-cli.py checkpoint create --description "CI test"

  benchmark:
    runs-on: ubuntu-latest
    needs: integration

    steps:
    - uses: actions/checkout@v3

    - name: Set up Python 3.10
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r scripts/requirements.txt

    - name: Performance benchmarks
      run: |
        cd scripts

        # Create performance test script
        cat > benchmark.py << 'EOF'
        import time
        import json
        from pathlib import Path

        # Create test data
        test_dir = Path(".claude/tasks")
        test_dir.mkdir(parents=True, exist_ok=True)

        # Create 100 test tasks
        for i in range(100):
            task = {
                "id": str(i),
                "title": f"Test Task {i}",
                "description": f"Description for task {i}",
                "difficulty": (i % 10) + 1,
                "status": ["Pending", "In Progress", "Finished"][i % 3],
                "created_date": "2025-12-17",
                "updated_date": "2025-12-17",
                "dependencies": [str(i-1)] if i > 0 else [],
                "confidence": 50 + (i % 50)
            }
            with open(test_dir / f"task-{i}.json", "w") as f:
                json.dump(task, f)

        # Benchmark operations
        from task_manager import TaskManager
        from validation_gates import ValidationGates
        from dependency_analyzer import DependencyAnalyzer

        manager = TaskManager(".")
        gates = ValidationGates(".")
        analyzer = DependencyAnalyzer(".")

        # Benchmark sync
        start = time.time()
        manager.sync_task_overview()
        sync_time = time.time() - start
        print(f"Sync 100 tasks: {sync_time:.3f}s")

        # Benchmark validation
        start = time.time()
        gates.validate_all_tasks()
        validation_time = time.time() - start
        print(f"Validate 100 tasks: {validation_time:.3f}s")

        # Benchmark dependency analysis
        start = time.time()
        analyzer.find_critical_path()
        analysis_time = time.time() - start
        print(f"Analyze dependencies: {analysis_time:.3f}s")

        # Assert performance targets
        assert sync_time < 1.0, "Sync too slow"
        assert validation_time < 2.0, "Validation too slow"
        assert analysis_time < 1.0, "Analysis too slow"

        print("All benchmarks passed!")
        EOF

        python benchmark.py

  deploy:
    runs-on: ubuntu-latest
    needs: [test, integration, benchmark]
    if: github.ref == 'refs/heads/main'

    steps:
    - uses: actions/checkout@v3

    - name: Package scripts
      run: |
        # Create distribution package
        tar -czf task-scripts.tar.gz scripts/

        # Create release notes
        cat > RELEASE.md << EOF
        # Task Management Scripts Release

        Version: $(date +%Y.%m.%d)

        ## Features
        - Core task management operations
        - Validation gates system
        - Bootstrap automation
        - Dependency analysis
        - Metrics dashboard
        - Unified CLI interface

        ## Installation
        \`\`\`bash
        tar -xzf task-scripts.tar.gz
        cd scripts
        pip install -r requirements.txt
        \`\`\`
        EOF

    - name: Create GitHub Release
      if: startsWith(github.ref, 'refs/tags/')
      uses: softprops/action-gh-release@v1
      with:
        files: |
          task-scripts.tar.gz
          RELEASE.md
        body_path: RELEASE.md
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}